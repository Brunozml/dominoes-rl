{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3d30a8",
   "metadata": {},
   "source": [
    "THIS NOTEBOOK NEEDS TO BE REVISITED AND UPDATED. IGNORE FOR NOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c0a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dominoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95c909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4923b096",
   "metadata": {},
   "source": [
    "# Tabular Q-Learning for dominoes\n",
    "in this notebook, we'll be developing a RL agent inside of the dominoes library.\n",
    "\n",
    "\n",
    "---\n",
    "**Notes to self**\n",
    "- [ ] trouble with move proposal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e8713",
   "metadata": {},
   "source": [
    "# 1. Observing the environment\n",
    "\n",
    "for now, we'll assume player 0 always starts. In the following block, a game is initialized and its state is displayed. We assume each player only knows her hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de102b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Board: [6|6]\n",
       "Player 0's hand: [0|4][0|3][2|4][2|6][4|6][3|6][3|4]\n",
       "Player 1's hand: [2|2][2|5][1|6][0|5][0|1][1|1][4|5]\n",
       "Player 2's hand: [5|6][0|0][1|4][3|3][0|2][1|3]\n",
       "Player 3's hand: [4|4][5|5][1|5][0|6][3|5][2|3][1|2]\n",
       "Player 3's turn"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#agent with be player 0\n",
    "game = dominoes.Game.new(starting_domino = dominoes.Domino(6, 6) )\n",
    "\n",
    "game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c12cd2",
   "metadata": {},
   "source": [
    "now, we display i) the state of the board and ii) the agents hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "757fef7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6|6], [0|5][2|6][2|2][1|1][1|6][1|3][2|3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.board, game.hands[0] # state of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf5575f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a72ed63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn = game.turn; turn # indicates which player is next (0-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53d16345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, [5|5][1|3][3|3][0|2][3|4][4|5][2|4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.turn, game.hands[game.turn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d9703",
   "metadata": {},
   "source": [
    "### building a simple state representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "304f4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_state(game):\n",
    "    return game.board.left_end(), game.board.right_end(), game.turn, game.hands\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "065dcc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " 6,\n",
       " 2,\n",
       " [[0|5][2|6][2|2][1|1][1|6][1|3][2|3],\n",
       "  [0|0][2|4][2|5][4|6][4|4][3|6],\n",
       "  [3|4][0|6][5|5][0|4][1|5][5|6][0|2],\n",
       "  [0|1][3|5][1|4][1|2][0|3][3|3][4|5]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_state(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e59f277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dominoes.Domino(6,3); d.second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61526d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Board: [0|6][6|6]\n",
       "Player 0's hand: [0|1][2|2][0|0][0|4][2|6][2|5]\n",
       "Player 1's hand: [0|3][1|1][1|5][4|4][0|5][3|5]\n",
       "Player 2's hand: [5|5][1|3][3|3][0|2][3|4][4|5][2|4]\n",
       "Player 3's hand: [1|2][1|4][2|3][5|6][3|6][4|6][1|6]\n",
       "Player 2's turn"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d364dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d769072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state):\n",
    "    \"\"\"\n",
    "    Important function! should implement as method.\n",
    "    uses game state to return list of available actions for the current\n",
    "    player\n",
    "    \"\"\"\n",
    "    left, right, turn, hands = state\n",
    "    actions = set()\n",
    "    for domino in hands[turn]:\n",
    "        if left in domino:\n",
    "            actions.add((domino, True))\n",
    "        if right in domino:\n",
    "            actions.add((domino, False))\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3871fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{([0|2], True)}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = simple_state(game)\n",
    "available_actions(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e003fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example move: ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8d3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a92b4e10",
   "metadata": {},
   "source": [
    "# 2. Executing an action\n",
    "\n",
    "here we take a look at exactly how an agent can take an action that influences the board.\n",
    "\n",
    "\n",
    "\n",
    "The following method provides the a tuple of the current player's dominoes and a boolean indicating whether it can be played or not. The automated players that have already been implemented basically just reorder this tuple of tuples. \n",
    "\n",
    "---\n",
    "I have some questions here:\n",
    "- Why is this done this way? could I do it another way?\n",
    "- what exactly does the valid_moves method do? \n",
    " - *it contains all of the moves in the hand of the current player. I don't really like how its implemented (its name is misleading).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de0870ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([0|6], True),)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before applying the player [ranking] algorithm\n",
    "game.valid_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57cae432",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.make_move(*game.valid_moves[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c9f124d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Board: [0|6][6|6]\n",
       "Player 0's hand: [0|1][2|2][0|0][0|4][2|6][2|5]\n",
       "Player 1's hand: [0|3][1|1][1|5][4|4][0|5][3|5]\n",
       "Player 2's hand: [5|5][1|3][3|3][0|2][3|4][4|5][2|4]\n",
       "Player 3's hand: [1|2][1|4][2|3][5|6][3|6][4|6][1|6]\n",
       "Player 2's turn"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7c512bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([0|2], True),)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.valid_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22eb9d",
   "metadata": {},
   "source": [
    "now we take a look at how some of the automated players produce there moves. In particular, we notice that when the player is called on the game object, it restructures the current player's hand according to its preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc876952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# automated players prefers to play highest domino.\n",
    "bota_gorda = dominoes.players.bota_gorda\n",
    "\n",
    "# apply the player setting to select a\n",
    "# move to play. the player setting is a callable that will sort the\n",
    "# game's valid moves in decreasing order of preference.\n",
    "bota_gorda(game) # can see them as game.valid_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5777cb2",
   "metadata": {},
   "source": [
    "the new ranking of the player's hand is now different...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29ac3540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([4|6], True), ([1|6], True))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after applying the player [ranking] algorithm\n",
    "game.valid_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d15445e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6|6], True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.valid_moves[0] \n",
    "\n",
    "# second value in tuple is the side to play. True is left, False is right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0ebb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the selected move\n",
    "\n",
    "game.make_move(*game.valid_moves[0]) # why the asterisk??? \n",
    "# asterisk argument => play the first move that is playable according to the \n",
    "# game rules. basically loops through the entire list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cfb48cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Board: [6|6]\n",
       "Player 0's hand: [3|6][1|1][5|5][1|5][1|3][1|2]\n",
       "Player 1's hand: [2|4][2|3][2|5][1|4][4|6][1|6][0|4]\n",
       "Player 2's hand: [0|1][3|5][5|6][4|5][0|5][3|4][0|3]\n",
       "Player 3's hand: [2|2][3|3][0|2][4|4][0|0][2|6][0|6]\n",
       "Player 1's turn"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec64c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acae5752",
   "metadata": {},
   "source": [
    "# 3. Building an agent\n",
    "\n",
    "- [X] determining the reward function\n",
    "- [ ] epsilon-greedy exploration algorithm\n",
    "- [ ] Q- learning\n",
    "\n",
    "## i.  Rewards\n",
    "\n",
    "Reward schedule:\n",
    "\n",
    "- Win: +points earned\n",
    "- Game continues: 0\n",
    "- Lose: -points earned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a1927d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(game):\n",
    "    \n",
    "    # game hasn't finished\n",
    "    if game.result is not None:\n",
    "        # if agent teams wins, reward = points earned\n",
    "        if game.result.player == 0 or game.result.player == 2:\n",
    "            # return 1\n",
    "            return game.result.points\n",
    "        # if agent teams wins, reward = -( points lost)\n",
    "        else:\n",
    "            # return -1\n",
    "            return -game.result.points\n",
    "    \n",
    "    # if game hasn't finished, do not return\n",
    "    return 0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "var points: the absolute value of this quantity indicates\n",
    "             the amount of points earned by the winning team.\n",
    "             This quantity is positive if the team consisting\n",
    "             of players 0 and 2 won, negative if the team\n",
    "             consisting of players 1 and 3 won, and 0 in case\n",
    "             of a tie.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c05e09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c0eda",
   "metadata": {},
   "source": [
    "## ii. e-greedy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02debbb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3269743613.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [53], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    def get_action(self, obs: tuple[int, int, bool]) -> int:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "### code obtained from \n",
    "### https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    " ### ---- will comment out algorithm for now\n",
    "    \n",
    "#     self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "#     def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \n",
    "#         \"\"\"\n",
    "#         Returns the best action with probability (1 - epsilon)\n",
    "#         otherwise a random action with probability epsilon to ensure exploration.\n",
    "#         \"\"\"\n",
    "#         # with probability epsilon return a random action to explore the environment\n",
    "#         if np.random.random() < self.epsilon:\n",
    "#             return env.action_space.sample()\n",
    "\n",
    "#         # with probability (1 - epsilon) act greedily (exploit)\n",
    "#         else:\n",
    "#             return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "#         ...\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e54d5",
   "metadata": {},
   "source": [
    "## iii. Q-learning\n",
    "\n",
    "- `self.q_values`: This is a dictionary that will store Q-values for each state-action pair in a reinforcement learning problem. The keys of this dictionary represent states, and the values are arrays representing the Q-values for each possible action in that state.\n",
    "\n",
    "- `defaultdict`: defaultdict is a type of dictionary provided by the collections module in Python. Here we're using defaultdict to ensure that if you access a state that hasn't been seen before, it will automatically create a new entry for that state with a default value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "177f99bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# code obtained from \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://gymnasium.farama.org/environments/toy_text/frozen_lake/\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mq_values \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: np\u001b[38;5;241m.\u001b[39mzeros(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# code obtained from \n",
    "# https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc984bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
